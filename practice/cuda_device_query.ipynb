{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se-UdA1YAQoW",
        "outputId": "67c75937-843f-4d21-e9d0-340cedd05a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJuubCYgDGGI",
        "outputId": "3c57f72c-0d7f-40b3-d67c-e1a770956128"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 22 17:34:08 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile deviceQuery.cu\n",
        "/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n",
        " *\n",
        " * Redistribution and use in source and binary forms, with or without\n",
        " * modification, are permitted provided that the following conditions\n",
        " * are met:\n",
        " *  * Redistributions of source code must retain the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer.\n",
        " *  * Redistributions in binary form must reproduce the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer in the\n",
        " *    documentation and/or other materials provided with the distribution.\n",
        " *  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
        " *    contributors may be used to endorse or promote products derived\n",
        " *    from this software without specific prior written permission.\n",
        " *\n",
        " * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
        " * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        " * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
        " * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
        " * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
        " * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
        " * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
        " * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
        " * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        " * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        " * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        " */\n",
        "\n",
        "/* This sample queries the properties of the CUDA devices present in the system\n",
        " * via CUDA Runtime API. */\n",
        "\n",
        "// std::system includes\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "#include <iostream>\n",
        "#include <memory>\n",
        "#include <string>\n",
        "\n",
        "int *pArgc = NULL;\n",
        "char **pArgv = NULL;\n",
        "\n",
        "#if CUDART_VERSION < 5000\n",
        "\n",
        "// CUDA-C includes\n",
        "#include <cuda.h>\n",
        "\n",
        "// This function wraps the CUDA Driver API into a template function\n",
        "template <class T>\n",
        "inline void getCudaAttribute(T *attribute, CUdevice_attribute device_attribute,\n",
        "                             int device) {\n",
        "  CUresult error = cuDeviceGetAttribute(attribute, device_attribute, device);\n",
        "\n",
        "  if (CUDA_SUCCESS != error) {\n",
        "    fprintf(\n",
        "        stderr,\n",
        "        \"cuSafeCallNoSync() Driver API error = %04d from file <%s>, line %i.\\n\",\n",
        "        error, __FILE__, __LINE__);\n",
        "\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "}\n",
        "\n",
        "#endif /* CUDART_VERSION < 5000 */\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "// Program main\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "int main(int argc, char **argv) {\n",
        "  pArgc = &argc;\n",
        "  pArgv = argv;\n",
        "\n",
        "  printf(\"%s Starting...\\n\\n\", argv[0]);\n",
        "  printf(\n",
        "      \" CUDA Device Query (Runtime API) version (CUDART static linking)\\n\\n\");\n",
        "\n",
        "  int deviceCount = 0;\n",
        "  cudaError_t error_id = cudaGetDeviceCount(&deviceCount);\n",
        "\n",
        "  if (error_id != cudaSuccess) {\n",
        "    printf(\"cudaGetDeviceCount returned %d\\n-> %s\\n\",\n",
        "           static_cast<int>(error_id), cudaGetErrorString(error_id));\n",
        "    printf(\"Result = FAIL\\n\");\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "\n",
        "  // This function call returns 0 if there are no CUDA capable devices.\n",
        "  if (deviceCount == 0) {\n",
        "    printf(\"There are no available device(s) that support CUDA\\n\");\n",
        "  } else {\n",
        "    printf(\"Detected %d CUDA Capable device(s)\\n\", deviceCount);\n",
        "  }\n",
        "\n",
        "  int dev, driverVersion = 0, runtimeVersion = 0;\n",
        "\n",
        "  for (dev = 0; dev < deviceCount; ++dev) {\n",
        "    cudaSetDevice(dev);\n",
        "    cudaDeviceProp deviceProp;\n",
        "    cudaGetDeviceProperties(&deviceProp, dev);\n",
        "\n",
        "    printf(\"\\nDevice %d: \\\"%s\\\"\\n\", dev, deviceProp.name);\n",
        "\n",
        "    // Console log\n",
        "    cudaDriverGetVersion(&driverVersion);\n",
        "    cudaRuntimeGetVersion(&runtimeVersion);\n",
        "    printf(\"  CUDA Driver Version / Runtime Version          %d.%d / %d.%d\\n\",\n",
        "           driverVersion / 1000, (driverVersion % 100) / 10,\n",
        "           runtimeVersion / 1000, (runtimeVersion % 100) / 10);\n",
        "    printf(\"  CUDA Capability Major/Minor version number:    %d.%d\\n\",\n",
        "           deviceProp.major, deviceProp.minor);\n",
        "\n",
        "    char msg[256];\n",
        "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
        "    sprintf_s(msg, sizeof(msg),\n",
        "              \"  Total amount of global memory:                 %.0f MBytes \"\n",
        "              \"(%llu bytes)\\n\",\n",
        "              static_cast<float>(deviceProp.totalGlobalMem / 1048576.0f),\n",
        "              (unsigned long long)deviceProp.totalGlobalMem);\n",
        "#else\n",
        "    snprintf(msg, sizeof(msg),\n",
        "             \"  Total amount of global memory:                 %.0f MBytes \"\n",
        "             \"(%llu bytes)\\n\",\n",
        "             static_cast<float>(deviceProp.totalGlobalMem / 1048576.0f),\n",
        "             (unsigned long long)deviceProp.totalGlobalMem);\n",
        "#endif\n",
        "    printf(\"%s\", msg);\n",
        "\n",
        "    printf(\"  (%03d) Multiprocessors, (%03d) CUDA Cores/MP:    %d CUDA Cores\\n\",\n",
        "           deviceProp.multiProcessorCount,\n",
        "           _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor),\n",
        "           _ConvertSMVer2Cores(deviceProp.major, deviceProp.minor) *\n",
        "               deviceProp.multiProcessorCount);\n",
        "    printf(\n",
        "        \"  GPU Max Clock rate:                            %.0f MHz (%0.2f \"\n",
        "        \"GHz)\\n\",\n",
        "        deviceProp.clockRate * 1e-3f, deviceProp.clockRate * 1e-6f);\n",
        "\n",
        "#if CUDART_VERSION >= 5000\n",
        "    // This is supported in CUDA 5.0 (runtime API device properties)\n",
        "    printf(\"  Memory Clock rate:                             %.0f Mhz\\n\",\n",
        "           deviceProp.memoryClockRate * 1e-3f);\n",
        "    printf(\"  Memory Bus Width:                              %d-bit\\n\",\n",
        "           deviceProp.memoryBusWidth);\n",
        "\n",
        "    if (deviceProp.l2CacheSize) {\n",
        "      printf(\"  L2 Cache Size:                                 %d bytes\\n\",\n",
        "             deviceProp.l2CacheSize);\n",
        "    }\n",
        "\n",
        "#else\n",
        "    // This only available in CUDA 4.0-4.2 (but these were only exposed in the\n",
        "    // CUDA Driver API)\n",
        "    int memoryClock;\n",
        "    getCudaAttribute<int>(&memoryClock, CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE,\n",
        "                          dev);\n",
        "    printf(\"  Memory Clock rate:                             %.0f Mhz\\n\",\n",
        "           memoryClock * 1e-3f);\n",
        "    int memBusWidth;\n",
        "    getCudaAttribute<int>(&memBusWidth,\n",
        "                          CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH, dev);\n",
        "    printf(\"  Memory Bus Width:                              %d-bit\\n\",\n",
        "           memBusWidth);\n",
        "    int L2CacheSize;\n",
        "    getCudaAttribute<int>(&L2CacheSize, CU_DEVICE_ATTRIBUTE_L2_CACHE_SIZE, dev);\n",
        "\n",
        "    if (L2CacheSize) {\n",
        "      printf(\"  L2 Cache Size:                                 %d bytes\\n\",\n",
        "             L2CacheSize);\n",
        "    }\n",
        "\n",
        "#endif\n",
        "\n",
        "    printf(\n",
        "        \"  Maximum Texture Dimension Size (x,y,z)         1D=(%d), 2D=(%d, \"\n",
        "        \"%d), 3D=(%d, %d, %d)\\n\",\n",
        "        deviceProp.maxTexture1D, deviceProp.maxTexture2D[0],\n",
        "        deviceProp.maxTexture2D[1], deviceProp.maxTexture3D[0],\n",
        "        deviceProp.maxTexture3D[1], deviceProp.maxTexture3D[2]);\n",
        "    printf(\n",
        "        \"  Maximum Layered 1D Texture Size, (num) layers  1D=(%d), %d layers\\n\",\n",
        "        deviceProp.maxTexture1DLayered[0], deviceProp.maxTexture1DLayered[1]);\n",
        "    printf(\n",
        "        \"  Maximum Layered 2D Texture Size, (num) layers  2D=(%d, %d), %d \"\n",
        "        \"layers\\n\",\n",
        "        deviceProp.maxTexture2DLayered[0], deviceProp.maxTexture2DLayered[1],\n",
        "        deviceProp.maxTexture2DLayered[2]);\n",
        "\n",
        "    printf(\"  Total amount of constant memory:               %zu bytes\\n\",\n",
        "           deviceProp.totalConstMem);\n",
        "    printf(\"  Total amount of shared memory per block:       %zu bytes\\n\",\n",
        "           deviceProp.sharedMemPerBlock);\n",
        "    printf(\"  Total shared memory per multiprocessor:        %zu bytes\\n\",\n",
        "           deviceProp.sharedMemPerMultiprocessor);\n",
        "    printf(\"  Total number of registers available per block: %d\\n\",\n",
        "           deviceProp.regsPerBlock);\n",
        "    printf(\"  Warp size:                                     %d\\n\",\n",
        "           deviceProp.warpSize);\n",
        "    printf(\"  Maximum number of threads per multiprocessor:  %d\\n\",\n",
        "           deviceProp.maxThreadsPerMultiProcessor);\n",
        "    printf(\"  Maximum number of threads per block:           %d\\n\",\n",
        "           deviceProp.maxThreadsPerBlock);\n",
        "    printf(\"  Max dimension size of a thread block (x,y,z): (%d, %d, %d)\\n\",\n",
        "           deviceProp.maxThreadsDim[0], deviceProp.maxThreadsDim[1],\n",
        "           deviceProp.maxThreadsDim[2]);\n",
        "    printf(\"  Max dimension size of a grid size    (x,y,z): (%d, %d, %d)\\n\",\n",
        "           deviceProp.maxGridSize[0], deviceProp.maxGridSize[1],\n",
        "           deviceProp.maxGridSize[2]);\n",
        "    printf(\"  Maximum memory pitch:                          %zu bytes\\n\",\n",
        "           deviceProp.memPitch);\n",
        "    printf(\"  Texture alignment:                             %zu bytes\\n\",\n",
        "           deviceProp.textureAlignment);\n",
        "    printf(\n",
        "        \"  Concurrent copy and kernel execution:          %s with %d copy \"\n",
        "        \"engine(s)\\n\",\n",
        "        (deviceProp.deviceOverlap ? \"Yes\" : \"No\"), deviceProp.asyncEngineCount);\n",
        "    printf(\"  Run time limit on kernels:                     %s\\n\",\n",
        "           deviceProp.kernelExecTimeoutEnabled ? \"Yes\" : \"No\");\n",
        "    printf(\"  Integrated GPU sharing Host Memory:            %s\\n\",\n",
        "           deviceProp.integrated ? \"Yes\" : \"No\");\n",
        "    printf(\"  Support host page-locked memory mapping:       %s\\n\",\n",
        "           deviceProp.canMapHostMemory ? \"Yes\" : \"No\");\n",
        "    printf(\"  Alignment requirement for Surfaces:            %s\\n\",\n",
        "           deviceProp.surfaceAlignment ? \"Yes\" : \"No\");\n",
        "    printf(\"  Device has ECC support:                        %s\\n\",\n",
        "           deviceProp.ECCEnabled ? \"Enabled\" : \"Disabled\");\n",
        "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
        "    printf(\"  CUDA Device Driver Mode (TCC or WDDM):         %s\\n\",\n",
        "           deviceProp.tccDriver ? \"TCC (Tesla Compute Cluster Driver)\"\n",
        "                                : \"WDDM (Windows Display Driver Model)\");\n",
        "#endif\n",
        "    printf(\"  Device supports Unified Addressing (UVA):      %s\\n\",\n",
        "           deviceProp.unifiedAddressing ? \"Yes\" : \"No\");\n",
        "    printf(\"  Device supports Managed Memory:                %s\\n\",\n",
        "           deviceProp.managedMemory ? \"Yes\" : \"No\");\n",
        "    printf(\"  Device supports Compute Preemption:            %s\\n\",\n",
        "           deviceProp.computePreemptionSupported ? \"Yes\" : \"No\");\n",
        "    printf(\"  Supports Cooperative Kernel Launch:            %s\\n\",\n",
        "           deviceProp.cooperativeLaunch ? \"Yes\" : \"No\");\n",
        "    printf(\"  Supports MultiDevice Co-op Kernel Launch:      %s\\n\",\n",
        "           deviceProp.cooperativeMultiDeviceLaunch ? \"Yes\" : \"No\");\n",
        "    printf(\"  Device PCI Domain ID / Bus ID / location ID:   %d / %d / %d\\n\",\n",
        "           deviceProp.pciDomainID, deviceProp.pciBusID, deviceProp.pciDeviceID);\n",
        "\n",
        "    const char *sComputeMode[] = {\n",
        "        \"Default (multiple host threads can use ::cudaSetDevice() with device \"\n",
        "        \"simultaneously)\",\n",
        "        \"Exclusive (only one host thread in one process is able to use \"\n",
        "        \"::cudaSetDevice() with this device)\",\n",
        "        \"Prohibited (no host thread can use ::cudaSetDevice() with this \"\n",
        "        \"device)\",\n",
        "        \"Exclusive Process (many threads in one process is able to use \"\n",
        "        \"::cudaSetDevice() with this device)\",\n",
        "        \"Unknown\", NULL};\n",
        "    printf(\"  Compute Mode:\\n\");\n",
        "    printf(\"     < %s >\\n\", sComputeMode[deviceProp.computeMode]);\n",
        "  }\n",
        "\n",
        "  // If there are 2 or more GPUs, query to determine whether RDMA is supported\n",
        "  if (deviceCount >= 2) {\n",
        "    cudaDeviceProp prop[64];\n",
        "    int gpuid[64];  // we want to find the first two GPUs that can support P2P\n",
        "    int gpu_p2p_count = 0;\n",
        "\n",
        "    for (int i = 0; i < deviceCount; i++) {\n",
        "      checkCudaErrors(cudaGetDeviceProperties(&prop[i], i));\n",
        "\n",
        "      // Only boards based on Fermi or later can support P2P\n",
        "      if ((prop[i].major >= 2)\n",
        "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
        "          // on Windows (64-bit), the Tesla Compute Cluster driver for windows\n",
        "          // must be enabled to support this\n",
        "          && prop[i].tccDriver\n",
        "#endif\n",
        "          ) {\n",
        "        // This is an array of P2P capable GPUs\n",
        "        gpuid[gpu_p2p_count++] = i;\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Show all the combinations of support P2P GPUs\n",
        "    int can_access_peer;\n",
        "\n",
        "    if (gpu_p2p_count >= 2) {\n",
        "      for (int i = 0; i < gpu_p2p_count; i++) {\n",
        "        for (int j = 0; j < gpu_p2p_count; j++) {\n",
        "          if (gpuid[i] == gpuid[j]) {\n",
        "            continue;\n",
        "          }\n",
        "          checkCudaErrors(\n",
        "              cudaDeviceCanAccessPeer(&can_access_peer, gpuid[i], gpuid[j]));\n",
        "          printf(\"> Peer access from %s (GPU%d) -> %s (GPU%d) : %s\\n\",\n",
        "                 prop[gpuid[i]].name, gpuid[i], prop[gpuid[j]].name, gpuid[j],\n",
        "                 can_access_peer ? \"Yes\" : \"No\");\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // csv masterlog info\n",
        "  // *****************************\n",
        "  // exe and CUDA driver name\n",
        "  printf(\"\\n\");\n",
        "  std::string sProfileString = \"deviceQuery, CUDA Driver = CUDART\";\n",
        "  char cTemp[16];\n",
        "\n",
        "  // driver version\n",
        "  sProfileString += \", CUDA Driver Version = \";\n",
        "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
        "  sprintf_s(cTemp, 10, \"%d.%d\", driverVersion / 1000,\n",
        "            (driverVersion % 100) / 10);\n",
        "#else\n",
        "  snprintf(cTemp, sizeof(cTemp), \"%d.%d\", driverVersion / 1000,\n",
        "           (driverVersion % 100) / 10);\n",
        "#endif\n",
        "  sProfileString += cTemp;\n",
        "\n",
        "  // Runtime version\n",
        "  sProfileString += \", CUDA Runtime Version = \";\n",
        "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
        "  sprintf_s(cTemp, 10, \"%d.%d\", runtimeVersion / 1000,\n",
        "            (runtimeVersion % 100) / 10);\n",
        "#else\n",
        "  snprintf(cTemp, sizeof(cTemp), \"%d.%d\", runtimeVersion / 1000,\n",
        "           (runtimeVersion % 100) / 10);\n",
        "#endif\n",
        "  sProfileString += cTemp;\n",
        "\n",
        "  // Device count\n",
        "  sProfileString += \", NumDevs = \";\n",
        "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
        "  sprintf_s(cTemp, 10, \"%d\", deviceCount);\n",
        "#else\n",
        "  snprintf(cTemp, sizeof(cTemp), \"%d\", deviceCount);\n",
        "#endif\n",
        "  sProfileString += cTemp;\n",
        "  sProfileString += \"\\n\";\n",
        "  printf(\"%s\", sProfileString.c_str());\n",
        "\n",
        "  printf(\"Result = PASS\\n\");\n",
        "\n",
        "  // finish\n",
        "  exit(EXIT_SUCCESS);\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebPn5a2SGYUF",
        "outputId": "15afc924-4b30-4e36-fca8-d1c69f5ff1e3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing deviceQuery.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile helper_cuda.h\n",
        "\n",
        "/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n",
        " *\n",
        " * Redistribution and use in source and binary forms, with or without\n",
        " * modification, are permitted provided that the following conditions\n",
        " * are met:\n",
        " *  * Redistributions of source code must retain the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer.\n",
        " *  * Redistributions in binary form must reproduce the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer in the\n",
        " *    documentation and/or other materials provided with the distribution.\n",
        " *  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
        " *    contributors may be used to endorse or promote products derived\n",
        " *    from this software without specific prior written permission.\n",
        " *\n",
        " * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
        " * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        " * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
        " * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
        " * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
        " * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
        " * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
        " * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
        " * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        " * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        " * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        " */\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////////////\n",
        "// These are CUDA Helper functions for initialization and error checking\n",
        "\n",
        "#ifndef COMMON_HELPER_CUDA_H_\n",
        "#define COMMON_HELPER_CUDA_H_\n",
        "\n",
        "#pragma once\n",
        "\n",
        "#include <stdint.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#include <helper_string.h>\n",
        "\n",
        "#ifndef EXIT_WAIVED\n",
        "#define EXIT_WAIVED 2\n",
        "#endif\n",
        "\n",
        "// Note, it is required that your SDK sample to include the proper header\n",
        "// files, please refer the CUDA examples for examples of the needed CUDA\n",
        "// headers, which may change depending on which CUDA functions are used.\n",
        "\n",
        "// CUDA Runtime error messages\n",
        "#ifdef __DRIVER_TYPES_H__\n",
        "static const char *_cudaGetErrorEnum(cudaError_t error) {\n",
        "  return cudaGetErrorName(error);\n",
        "}\n",
        "#endif\n",
        "\n",
        "#ifdef CUDA_DRIVER_API\n",
        "// CUDA Driver API errors\n",
        "static const char *_cudaGetErrorEnum(CUresult error) {\n",
        "  static char unknown[] = \"<unknown>\";\n",
        "  const char *ret = NULL;\n",
        "  cuGetErrorName(error, &ret);\n",
        "  return ret ? ret : unknown;\n",
        "}\n",
        "#endif\n",
        "\n",
        "#ifdef CUBLAS_API_H_\n",
        "// cuBLAS API errors\n",
        "static const char *_cudaGetErrorEnum(cublasStatus_t error) {\n",
        "  switch (error) {\n",
        "    case CUBLAS_STATUS_SUCCESS:\n",
        "      return \"CUBLAS_STATUS_SUCCESS\";\n",
        "\n",
        "    case CUBLAS_STATUS_NOT_INITIALIZED:\n",
        "      return \"CUBLAS_STATUS_NOT_INITIALIZED\";\n",
        "\n",
        "    case CUBLAS_STATUS_ALLOC_FAILED:\n",
        "      return \"CUBLAS_STATUS_ALLOC_FAILED\";\n",
        "\n",
        "    case CUBLAS_STATUS_INVALID_VALUE:\n",
        "      return \"CUBLAS_STATUS_INVALID_VALUE\";\n",
        "\n",
        "    case CUBLAS_STATUS_ARCH_MISMATCH:\n",
        "      return \"CUBLAS_STATUS_ARCH_MISMATCH\";\n",
        "\n",
        "    case CUBLAS_STATUS_MAPPING_ERROR:\n",
        "      return \"CUBLAS_STATUS_MAPPING_ERROR\";\n",
        "\n",
        "    case CUBLAS_STATUS_EXECUTION_FAILED:\n",
        "      return \"CUBLAS_STATUS_EXECUTION_FAILED\";\n",
        "\n",
        "    case CUBLAS_STATUS_INTERNAL_ERROR:\n",
        "      return \"CUBLAS_STATUS_INTERNAL_ERROR\";\n",
        "\n",
        "    case CUBLAS_STATUS_NOT_SUPPORTED:\n",
        "      return \"CUBLAS_STATUS_NOT_SUPPORTED\";\n",
        "\n",
        "    case CUBLAS_STATUS_LICENSE_ERROR:\n",
        "      return \"CUBLAS_STATUS_LICENSE_ERROR\";\n",
        "  }\n",
        "\n",
        "  return \"<unknown>\";\n",
        "}\n",
        "#endif\n",
        "\n",
        "#ifdef _CUFFT_H_\n",
        "// cuFFT API errors\n",
        "static const char *_cudaGetErrorEnum(cufftResult error) {\n",
        "  switch (error) {\n",
        "    case CUFFT_SUCCESS:\n",
        "      return \"CUFFT_SUCCESS\";\n",
        "\n",
        "    case CUFFT_INVALID_PLAN:\n",
        "      return \"CUFFT_INVALID_PLAN\";\n",
        "\n",
        "    case CUFFT_ALLOC_FAILED:\n",
        "      return \"CUFFT_ALLOC_FAILED\";\n",
        "\n",
        "    case CUFFT_INVALID_TYPE:\n",
        "      return \"CUFFT_INVALID_TYPE\";\n",
        "\n",
        "    case CUFFT_INVALID_VALUE:\n",
        "      return \"CUFFT_INVALID_VALUE\";\n",
        "\n",
        "    case CUFFT_INTERNAL_ERROR:\n",
        "      return \"CUFFT_INTERNAL_ERROR\";\n",
        "\n",
        "    case CUFFT_EXEC_FAILED:\n",
        "      return \"CUFFT_EXEC_FAILED\";\n",
        "\n",
        "    case CUFFT_SETUP_FAILED:\n",
        "      return \"CUFFT_SETUP_FAILED\";\n",
        "\n",
        "    case CUFFT_INVALID_SIZE:\n",
        "      return \"CUFFT_INVALID_SIZE\";\n",
        "\n",
        "    case CUFFT_UNALIGNED_DATA:\n",
        "      return \"CUFFT_UNALIGNED_DATA\";\n",
        "\n",
        "    case CUFFT_INCOMPLETE_PARAMETER_LIST:\n",
        "      return \"CUFFT_INCOMPLETE_PARAMETER_LIST\";\n",
        "\n",
        "    case CUFFT_INVALID_DEVICE:\n",
        "      return \"CUFFT_INVALID_DEVICE\";\n",
        "\n",
        "    case CUFFT_PARSE_ERROR:\n",
        "      return \"CUFFT_PARSE_ERROR\";\n",
        "\n",
        "    case CUFFT_NO_WORKSPACE:\n",
        "      return \"CUFFT_NO_WORKSPACE\";\n",
        "\n",
        "    case CUFFT_NOT_IMPLEMENTED:\n",
        "      return \"CUFFT_NOT_IMPLEMENTED\";\n",
        "\n",
        "    case CUFFT_LICENSE_ERROR:\n",
        "      return \"CUFFT_LICENSE_ERROR\";\n",
        "\n",
        "    case CUFFT_NOT_SUPPORTED:\n",
        "      return \"CUFFT_NOT_SUPPORTED\";\n",
        "  }\n",
        "\n",
        "  return \"<unknown>\";\n",
        "}\n",
        "#endif\n",
        "\n",
        "#ifdef CUSPARSEAPI\n",
        "// cuSPARSE API errors\n",
        "static const char *_cudaGetErrorEnum(cusparseStatus_t error) {\n",
        "  switch (error) {\n",
        "    case CUSPARSE_STATUS_SUCCESS:\n",
        "      return \"CUSPARSE_STATUS_SUCCESS\";\n",
        "\n",
        "    case CUSPARSE_STATUS_NOT_INITIALIZED:\n",
        "      return \"CUSPARSE_STATUS_NOT_INITIALIZED\";\n",
        "\n",
        "    case CUSPARSE_STATUS_ALLOC_FAILED:\n",
        "      return \"CUSPARSE_STATUS_ALLOC_FAILED\";\n",
        "\n",
        "    case CUSPARSE_STATUS_INVALID_VALUE:\n",
        "      return \"CUSPARSE_STATUS_INVALID_VALUE\";\n",
        "\n",
        "    case CUSPARSE_STATUS_ARCH_MISMATCH:\n",
        "      return \"CUSPARSE_STATUS_ARCH_MISMATCH\";\n",
        "\n",
        "    case CUSPARSE_STATUS_MAPPING_ERROR:\n",
        "      return \"CUSPARSE_STATUS_MAPPING_ERROR\";\n",
        "\n",
        "    case CUSPARSE_STATUS_EXECUTION_FAILED:\n",
        "      return \"CUSPARSE_STATUS_EXECUTION_FAILED\";\n",
        "\n",
        "    case CUSPARSE_STATUS_INTERNAL_ERROR:\n",
        "      return \"CUSPARSE_STATUS_INTERNAL_ERROR\";\n",
        "\n",
        "    case CUSPARSE_STATUS_MATRIX_TYPE_NOT_SUPPORTED:\n",
        "      return \"CUSPARSE_STATUS_MATRIX_TYPE_NOT_SUPPORTED\";\n",
        "  }\n",
        "\n",
        "  return \"<unknown>\";\n",
        "}\n",
        "#endif\n",
        "\n",
        "#ifdef CUSOLVER_COMMON_H_\n",
        "// cuSOLVER API errors\n",
        "static const char *_cudaGetErrorEnum(cusolverStatus_t error) {\n",
        "  switch (error) {\n",
        "    case CUSOLVER_STATUS_SUCCESS:\n",
        "      return \"CUSOLVER_STATUS_SUCCESS\";\n",
        "    case CUSOLVER_STATUS_NOT_INITIALIZED:\n",
        "      return \"CUSOLVER_STATUS_NOT_INITIALIZED\";\n",
        "    case CUSOLVER_STATUS_ALLOC_FAILED:\n",
        "      return \"CUSOLVER_STATUS_ALLOC_FAILED\";\n",
        "    case CUSOLVER_STATUS_INVALID_VALUE:\n",
        "      return \"CUSOLVER_STATUS_INVALID_VALUE\";\n",
        "    case CUSOLVER_STATUS_ARCH_MISMATCH:\n",
        "      return \"CUSOLVER_STATUS_ARCH_MISMATCH\";\n",
        "    case CUSOLVER_STATUS_MAPPING_ERROR:\n",
        "      return \"CUSOLVER_STATUS_MAPPING_ERROR\";\n",
        "    case CUSOLVER_STATUS_EXECUTION_FAILED:\n",
        "      return \"CUSOLVER_STATUS_EXECUTION_FAILED\";\n",
        "    case CUSOLVER_STATUS_INTERNAL_ERROR:\n",
        "      return \"CUSOLVER_STATUS_INTERNAL_ERROR\";\n",
        "    case CUSOLVER_STATUS_MATRIX_TYPE_NOT_SUPPORTED:\n",
        "      return \"CUSOLVER_STATUS_MATRIX_TYPE_NOT_SUPPORTED\";\n",
        "    case CUSOLVER_STATUS_NOT_SUPPORTED:\n",
        "      return \"CUSOLVER_STATUS_NOT_SUPPORTED \";\n",
        "    case CUSOLVER_STATUS_ZERO_PIVOT:\n",
        "      return \"CUSOLVER_STATUS_ZERO_PIVOT\";\n",
        "    case CUSOLVER_STATUS_INVALID_LICENSE:\n",
        "      return \"CUSOLVER_STATUS_INVALID_LICENSE\";\n",
        "  }\n",
        "\n",
        "  return \"<unknown>\";\n",
        "}\n",
        "#endif\n",
        "\n",
        "#ifdef CURAND_H_\n",
        "// cuRAND API errors\n",
        "static const char *_cudaGetErrorEnum(curandStatus_t error) {\n",
        "  switch (error) {\n",
        "    case CURAND_STATUS_SUCCESS:\n",
        "      return \"CURAND_STATUS_SUCCESS\";\n",
        "\n",
        "    case CURAND_STATUS_VERSION_MISMATCH:\n",
        "      return \"CURAND_STATUS_VERSION_MISMATCH\";\n",
        "\n",
        "    case CURAND_STATUS_NOT_INITIALIZED:\n",
        "      return \"CURAND_STATUS_NOT_INITIALIZED\";\n",
        "\n",
        "    case CURAND_STATUS_ALLOCATION_FAILED:\n",
        "      return \"CURAND_STATUS_ALLOCATION_FAILED\";\n",
        "\n",
        "    case CURAND_STATUS_TYPE_ERROR:\n",
        "      return \"CURAND_STATUS_TYPE_ERROR\";\n",
        "\n",
        "    case CURAND_STATUS_OUT_OF_RANGE:\n",
        "      return \"CURAND_STATUS_OUT_OF_RANGE\";\n",
        "\n",
        "    case CURAND_STATUS_LENGTH_NOT_MULTIPLE:\n",
        "      return \"CURAND_STATUS_LENGTH_NOT_MULTIPLE\";\n",
        "\n",
        "    case CURAND_STATUS_DOUBLE_PRECISION_REQUIRED:\n",
        "      return \"CURAND_STATUS_DOUBLE_PRECISION_REQUIRED\";\n",
        "\n",
        "    case CURAND_STATUS_LAUNCH_FAILURE:\n",
        "      return \"CURAND_STATUS_LAUNCH_FAILURE\";\n",
        "\n",
        "    case CURAND_STATUS_PREEXISTING_FAILURE:\n",
        "      return \"CURAND_STATUS_PREEXISTING_FAILURE\";\n",
        "\n",
        "    case CURAND_STATUS_INITIALIZATION_FAILED:\n",
        "      return \"CURAND_STATUS_INITIALIZATION_FAILED\";\n",
        "\n",
        "    case CURAND_STATUS_ARCH_MISMATCH:\n",
        "      return \"CURAND_STATUS_ARCH_MISMATCH\";\n",
        "\n",
        "    case CURAND_STATUS_INTERNAL_ERROR:\n",
        "      return \"CURAND_STATUS_INTERNAL_ERROR\";\n",
        "  }\n",
        "\n",
        "  return \"<unknown>\";\n",
        "}\n",
        "#endif\n",
        "\n",
        "#ifdef NVJPEGAPI\n",
        "// nvJPEG API errors\n",
        "static const char *_cudaGetErrorEnum(nvjpegStatus_t error) {\n",
        "  switch (error) {\n",
        "    case NVJPEG_STATUS_SUCCESS:\n",
        "      return \"NVJPEG_STATUS_SUCCESS\";\n",
        "\n",
        "    case NVJPEG_STATUS_NOT_INITIALIZED:\n",
        "      return \"NVJPEG_STATUS_NOT_INITIALIZED\";\n",
        "\n",
        "    case NVJPEG_STATUS_INVALID_PARAMETER:\n",
        "      return \"NVJPEG_STATUS_INVALID_PARAMETER\";\n",
        "\n",
        "    case NVJPEG_STATUS_BAD_JPEG:\n",
        "      return \"NVJPEG_STATUS_BAD_JPEG\";\n",
        "\n",
        "    case NVJPEG_STATUS_JPEG_NOT_SUPPORTED:\n",
        "      return \"NVJPEG_STATUS_JPEG_NOT_SUPPORTED\";\n",
        "\n",
        "    case NVJPEG_STATUS_ALLOCATOR_FAILURE:\n",
        "      return \"NVJPEG_STATUS_ALLOCATOR_FAILURE\";\n",
        "\n",
        "    case NVJPEG_STATUS_EXECUTION_FAILED:\n",
        "      return \"NVJPEG_STATUS_EXECUTION_FAILED\";\n",
        "\n",
        "    case NVJPEG_STATUS_ARCH_MISMATCH:\n",
        "      return \"NVJPEG_STATUS_ARCH_MISMATCH\";\n",
        "\n",
        "    case NVJPEG_STATUS_INTERNAL_ERROR:\n",
        "      return \"NVJPEG_STATUS_INTERNAL_ERROR\";\n",
        "  }\n",
        "\n",
        "  return \"<unknown>\";\n",
        "}\n",
        "#endif\n",
        "\n",
        "#ifdef NV_NPPIDEFS_H\n",
        "// NPP API errors\n",
        "static const char *_cudaGetErrorEnum(NppStatus error) {\n",
        "  switch (error) {\n",
        "    case NPP_NOT_SUPPORTED_MODE_ERROR:\n",
        "      return \"NPP_NOT_SUPPORTED_MODE_ERROR\";\n",
        "\n",
        "    case NPP_ROUND_MODE_NOT_SUPPORTED_ERROR:\n",
        "      return \"NPP_ROUND_MODE_NOT_SUPPORTED_ERROR\";\n",
        "\n",
        "    case NPP_RESIZE_NO_OPERATION_ERROR:\n",
        "      return \"NPP_RESIZE_NO_OPERATION_ERROR\";\n",
        "\n",
        "    case NPP_NOT_SUFFICIENT_COMPUTE_CAPABILITY:\n",
        "      return \"NPP_NOT_SUFFICIENT_COMPUTE_CAPABILITY\";\n",
        "\n",
        "#if ((NPP_VERSION_MAJOR << 12) + (NPP_VERSION_MINOR << 4)) <= 0x5000\n",
        "\n",
        "    case NPP_BAD_ARG_ERROR:\n",
        "      return \"NPP_BAD_ARGUMENT_ERROR\";\n",
        "\n",
        "    case NPP_COEFF_ERROR:\n",
        "      return \"NPP_COEFFICIENT_ERROR\";\n",
        "\n",
        "    case NPP_RECT_ERROR:\n",
        "      return \"NPP_RECTANGLE_ERROR\";\n",
        "\n",
        "    case NPP_QUAD_ERROR:\n",
        "      return \"NPP_QUADRANGLE_ERROR\";\n",
        "\n",
        "    case NPP_MEM_ALLOC_ERR:\n",
        "      return \"NPP_MEMORY_ALLOCATION_ERROR\";\n",
        "\n",
        "    case NPP_HISTO_NUMBER_OF_LEVELS_ERROR:\n",
        "      return \"NPP_HISTOGRAM_NUMBER_OF_LEVELS_ERROR\";\n",
        "\n",
        "    case NPP_INVALID_INPUT:\n",
        "      return \"NPP_INVALID_INPUT\";\n",
        "\n",
        "    case NPP_POINTER_ERROR:\n",
        "      return \"NPP_POINTER_ERROR\";\n",
        "\n",
        "    case NPP_WARNING:\n",
        "      return \"NPP_WARNING\";\n",
        "\n",
        "    case NPP_ODD_ROI_WARNING:\n",
        "      return \"NPP_ODD_ROI_WARNING\";\n",
        "#else\n",
        "\n",
        "    // These are for CUDA 5.5 or higher\n",
        "    case NPP_BAD_ARGUMENT_ERROR:\n",
        "      return \"NPP_BAD_ARGUMENT_ERROR\";\n",
        "\n",
        "    case NPP_COEFFICIENT_ERROR:\n",
        "      return \"NPP_COEFFICIENT_ERROR\";\n",
        "\n",
        "    case NPP_RECTANGLE_ERROR:\n",
        "      return \"NPP_RECTANGLE_ERROR\";\n",
        "\n",
        "    case NPP_QUADRANGLE_ERROR:\n",
        "      return \"NPP_QUADRANGLE_ERROR\";\n",
        "\n",
        "    case NPP_MEMORY_ALLOCATION_ERR:\n",
        "      return \"NPP_MEMORY_ALLOCATION_ERROR\";\n",
        "\n",
        "    case NPP_HISTOGRAM_NUMBER_OF_LEVELS_ERROR:\n",
        "      return \"NPP_HISTOGRAM_NUMBER_OF_LEVELS_ERROR\";\n",
        "\n",
        "    case NPP_INVALID_HOST_POINTER_ERROR:\n",
        "      return \"NPP_INVALID_HOST_POINTER_ERROR\";\n",
        "\n",
        "    case NPP_INVALID_DEVICE_POINTER_ERROR:\n",
        "      return \"NPP_INVALID_DEVICE_POINTER_ERROR\";\n",
        "#endif\n",
        "\n",
        "    case NPP_LUT_NUMBER_OF_LEVELS_ERROR:\n",
        "      return \"NPP_LUT_NUMBER_OF_LEVELS_ERROR\";\n",
        "\n",
        "    case NPP_TEXTURE_BIND_ERROR:\n",
        "      return \"NPP_TEXTURE_BIND_ERROR\";\n",
        "\n",
        "    case NPP_WRONG_INTERSECTION_ROI_ERROR:\n",
        "      return \"NPP_WRONG_INTERSECTION_ROI_ERROR\";\n",
        "\n",
        "    case NPP_NOT_EVEN_STEP_ERROR:\n",
        "      return \"NPP_NOT_EVEN_STEP_ERROR\";\n",
        "\n",
        "    case NPP_INTERPOLATION_ERROR:\n",
        "      return \"NPP_INTERPOLATION_ERROR\";\n",
        "\n",
        "    case NPP_RESIZE_FACTOR_ERROR:\n",
        "      return \"NPP_RESIZE_FACTOR_ERROR\";\n",
        "\n",
        "    case NPP_HAAR_CLASSIFIER_PIXEL_MATCH_ERROR:\n",
        "      return \"NPP_HAAR_CLASSIFIER_PIXEL_MATCH_ERROR\";\n",
        "\n",
        "#if ((NPP_VERSION_MAJOR << 12) + (NPP_VERSION_MINOR << 4)) <= 0x5000\n",
        "\n",
        "    case NPP_MEMFREE_ERR:\n",
        "      return \"NPP_MEMFREE_ERR\";\n",
        "\n",
        "    case NPP_MEMSET_ERR:\n",
        "      return \"NPP_MEMSET_ERR\";\n",
        "\n",
        "    case NPP_MEMCPY_ERR:\n",
        "      return \"NPP_MEMCPY_ERROR\";\n",
        "\n",
        "    case NPP_MIRROR_FLIP_ERR:\n",
        "      return \"NPP_MIRROR_FLIP_ERR\";\n",
        "#else\n",
        "\n",
        "    case NPP_MEMFREE_ERROR:\n",
        "      return \"NPP_MEMFREE_ERROR\";\n",
        "\n",
        "    case NPP_MEMSET_ERROR:\n",
        "      return \"NPP_MEMSET_ERROR\";\n",
        "\n",
        "    case NPP_MEMCPY_ERROR:\n",
        "      return \"NPP_MEMCPY_ERROR\";\n",
        "\n",
        "    case NPP_MIRROR_FLIP_ERROR:\n",
        "      return \"NPP_MIRROR_FLIP_ERROR\";\n",
        "#endif\n",
        "\n",
        "    case NPP_ALIGNMENT_ERROR:\n",
        "      return \"NPP_ALIGNMENT_ERROR\";\n",
        "\n",
        "    case NPP_STEP_ERROR:\n",
        "      return \"NPP_STEP_ERROR\";\n",
        "\n",
        "    case NPP_SIZE_ERROR:\n",
        "      return \"NPP_SIZE_ERROR\";\n",
        "\n",
        "    case NPP_NULL_POINTER_ERROR:\n",
        "      return \"NPP_NULL_POINTER_ERROR\";\n",
        "\n",
        "    case NPP_CUDA_KERNEL_EXECUTION_ERROR:\n",
        "      return \"NPP_CUDA_KERNEL_EXECUTION_ERROR\";\n",
        "\n",
        "    case NPP_NOT_IMPLEMENTED_ERROR:\n",
        "      return \"NPP_NOT_IMPLEMENTED_ERROR\";\n",
        "\n",
        "    case NPP_ERROR:\n",
        "      return \"NPP_ERROR\";\n",
        "\n",
        "    case NPP_SUCCESS:\n",
        "      return \"NPP_SUCCESS\";\n",
        "\n",
        "    case NPP_WRONG_INTERSECTION_QUAD_WARNING:\n",
        "      return \"NPP_WRONG_INTERSECTION_QUAD_WARNING\";\n",
        "\n",
        "    case NPP_MISALIGNED_DST_ROI_WARNING:\n",
        "      return \"NPP_MISALIGNED_DST_ROI_WARNING\";\n",
        "\n",
        "    case NPP_AFFINE_QUAD_INCORRECT_WARNING:\n",
        "      return \"NPP_AFFINE_QUAD_INCORRECT_WARNING\";\n",
        "\n",
        "    case NPP_DOUBLE_SIZE_WARNING:\n",
        "      return \"NPP_DOUBLE_SIZE_WARNING\";\n",
        "\n",
        "    case NPP_WRONG_INTERSECTION_ROI_WARNING:\n",
        "      return \"NPP_WRONG_INTERSECTION_ROI_WARNING\";\n",
        "\n",
        "#if ((NPP_VERSION_MAJOR << 12) + (NPP_VERSION_MINOR << 4)) >= 0x6000\n",
        "    /* These are 6.0 or higher */\n",
        "    case NPP_LUT_PALETTE_BITSIZE_ERROR:\n",
        "      return \"NPP_LUT_PALETTE_BITSIZE_ERROR\";\n",
        "\n",
        "    case NPP_ZC_MODE_NOT_SUPPORTED_ERROR:\n",
        "      return \"NPP_ZC_MODE_NOT_SUPPORTED_ERROR\";\n",
        "\n",
        "    case NPP_QUALITY_INDEX_ERROR:\n",
        "      return \"NPP_QUALITY_INDEX_ERROR\";\n",
        "\n",
        "    case NPP_CHANNEL_ORDER_ERROR:\n",
        "      return \"NPP_CHANNEL_ORDER_ERROR\";\n",
        "\n",
        "    case NPP_ZERO_MASK_VALUE_ERROR:\n",
        "      return \"NPP_ZERO_MASK_VALUE_ERROR\";\n",
        "\n",
        "    case NPP_NUMBER_OF_CHANNELS_ERROR:\n",
        "      return \"NPP_NUMBER_OF_CHANNELS_ERROR\";\n",
        "\n",
        "    case NPP_COI_ERROR:\n",
        "      return \"NPP_COI_ERROR\";\n",
        "\n",
        "    case NPP_DIVISOR_ERROR:\n",
        "      return \"NPP_DIVISOR_ERROR\";\n",
        "\n",
        "    case NPP_CHANNEL_ERROR:\n",
        "      return \"NPP_CHANNEL_ERROR\";\n",
        "\n",
        "    case NPP_STRIDE_ERROR:\n",
        "      return \"NPP_STRIDE_ERROR\";\n",
        "\n",
        "    case NPP_ANCHOR_ERROR:\n",
        "      return \"NPP_ANCHOR_ERROR\";\n",
        "\n",
        "    case NPP_MASK_SIZE_ERROR:\n",
        "      return \"NPP_MASK_SIZE_ERROR\";\n",
        "\n",
        "    case NPP_MOMENT_00_ZERO_ERROR:\n",
        "      return \"NPP_MOMENT_00_ZERO_ERROR\";\n",
        "\n",
        "    case NPP_THRESHOLD_NEGATIVE_LEVEL_ERROR:\n",
        "      return \"NPP_THRESHOLD_NEGATIVE_LEVEL_ERROR\";\n",
        "\n",
        "    case NPP_THRESHOLD_ERROR:\n",
        "      return \"NPP_THRESHOLD_ERROR\";\n",
        "\n",
        "    case NPP_CONTEXT_MATCH_ERROR:\n",
        "      return \"NPP_CONTEXT_MATCH_ERROR\";\n",
        "\n",
        "    case NPP_FFT_FLAG_ERROR:\n",
        "      return \"NPP_FFT_FLAG_ERROR\";\n",
        "\n",
        "    case NPP_FFT_ORDER_ERROR:\n",
        "      return \"NPP_FFT_ORDER_ERROR\";\n",
        "\n",
        "    case NPP_SCALE_RANGE_ERROR:\n",
        "      return \"NPP_SCALE_RANGE_ERROR\";\n",
        "\n",
        "    case NPP_DATA_TYPE_ERROR:\n",
        "      return \"NPP_DATA_TYPE_ERROR\";\n",
        "\n",
        "    case NPP_OUT_OFF_RANGE_ERROR:\n",
        "      return \"NPP_OUT_OFF_RANGE_ERROR\";\n",
        "\n",
        "    case NPP_DIVIDE_BY_ZERO_ERROR:\n",
        "      return \"NPP_DIVIDE_BY_ZERO_ERROR\";\n",
        "\n",
        "    case NPP_RANGE_ERROR:\n",
        "      return \"NPP_RANGE_ERROR\";\n",
        "\n",
        "    case NPP_NO_MEMORY_ERROR:\n",
        "      return \"NPP_NO_MEMORY_ERROR\";\n",
        "\n",
        "    case NPP_ERROR_RESERVED:\n",
        "      return \"NPP_ERROR_RESERVED\";\n",
        "\n",
        "    case NPP_NO_OPERATION_WARNING:\n",
        "      return \"NPP_NO_OPERATION_WARNING\";\n",
        "\n",
        "    case NPP_DIVIDE_BY_ZERO_WARNING:\n",
        "      return \"NPP_DIVIDE_BY_ZERO_WARNING\";\n",
        "#endif\n",
        "\n",
        "#if ((NPP_VERSION_MAJOR << 12) + (NPP_VERSION_MINOR << 4)) >= 0x7000\n",
        "    /* These are 7.0 or higher */\n",
        "    case NPP_OVERFLOW_ERROR:\n",
        "      return \"NPP_OVERFLOW_ERROR\";\n",
        "\n",
        "    case NPP_CORRUPTED_DATA_ERROR:\n",
        "      return \"NPP_CORRUPTED_DATA_ERROR\";\n",
        "#endif\n",
        "  }\n",
        "\n",
        "  return \"<unknown>\";\n",
        "}\n",
        "#endif\n",
        "\n",
        "template <typename T>\n",
        "void check(T result, char const *const func, const char *const file,\n",
        "           int const line) {\n",
        "  if (result) {\n",
        "    fprintf(stderr, \"CUDA error at %s:%d code=%d(%s) \\\"%s\\\" \\n\", file, line,\n",
        "            static_cast<unsigned int>(result), _cudaGetErrorEnum(result), func);\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "}\n",
        "\n",
        "#ifdef __DRIVER_TYPES_H__\n",
        "// This will output the proper CUDA error strings in the event\n",
        "// that a CUDA host call returns an error\n",
        "#define checkCudaErrors(val) check((val), #val, __FILE__, __LINE__)\n",
        "\n",
        "// This will output the proper error string when calling cudaGetLastError\n",
        "#define getLastCudaError(msg) __getLastCudaError(msg, __FILE__, __LINE__)\n",
        "\n",
        "inline void __getLastCudaError(const char *errorMessage, const char *file,\n",
        "                               const int line) {\n",
        "  cudaError_t err = cudaGetLastError();\n",
        "\n",
        "  if (cudaSuccess != err) {\n",
        "    fprintf(stderr,\n",
        "            \"%s(%i) : getLastCudaError() CUDA error :\"\n",
        "            \" %s : (%d) %s.\\n\",\n",
        "            file, line, errorMessage, static_cast<int>(err),\n",
        "            cudaGetErrorString(err));\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "}\n",
        "\n",
        "// This will only print the proper error string when calling cudaGetLastError\n",
        "// but not exit program incase error detected.\n",
        "#define printLastCudaError(msg) __printLastCudaError(msg, __FILE__, __LINE__)\n",
        "\n",
        "inline void __printLastCudaError(const char *errorMessage, const char *file,\n",
        "                                 const int line) {\n",
        "  cudaError_t err = cudaGetLastError();\n",
        "\n",
        "  if (cudaSuccess != err) {\n",
        "    fprintf(stderr,\n",
        "            \"%s(%i) : getLastCudaError() CUDA error :\"\n",
        "            \" %s : (%d) %s.\\n\",\n",
        "            file, line, errorMessage, static_cast<int>(err),\n",
        "            cudaGetErrorString(err));\n",
        "  }\n",
        "}\n",
        "#endif\n",
        "\n",
        "#ifndef MAX\n",
        "#define MAX(a, b) (a > b ? a : b)\n",
        "#endif\n",
        "\n",
        "// Float To Int conversion\n",
        "inline int ftoi(float value) {\n",
        "  return (value >= 0 ? static_cast<int>(value + 0.5)\n",
        "                     : static_cast<int>(value - 0.5));\n",
        "}\n",
        "\n",
        "// Beginning of GPU Architecture definitions\n",
        "inline int _ConvertSMVer2Cores(int major, int minor) {\n",
        "  // Defines for GPU Architecture types (using the SM version to determine\n",
        "  // the # of cores per SM\n",
        "  typedef struct {\n",
        "    int SM;  // 0xMm (hexidecimal notation), M = SM Major version,\n",
        "    // and m = SM minor version\n",
        "    int Cores;\n",
        "  } sSMtoCores;\n",
        "\n",
        "  sSMtoCores nGpuArchCoresPerSM[] = {\n",
        "      {0x30, 192},\n",
        "      {0x32, 192},\n",
        "      {0x35, 192},\n",
        "      {0x37, 192},\n",
        "      {0x50, 128},\n",
        "      {0x52, 128},\n",
        "      {0x53, 128},\n",
        "      {0x60,  64},\n",
        "      {0x61, 128},\n",
        "      {0x62, 128},\n",
        "      {0x70,  64},\n",
        "      {0x72,  64},\n",
        "      {0x75,  64},\n",
        "      {0x80,  64},\n",
        "      {0x86, 128},\n",
        "      {0x87, 128},\n",
        "      {0x89, 128},\n",
        "      {0x90, 128},\n",
        "      {-1, -1}};\n",
        "\n",
        "  int index = 0;\n",
        "\n",
        "  while (nGpuArchCoresPerSM[index].SM != -1) {\n",
        "    if (nGpuArchCoresPerSM[index].SM == ((major << 4) + minor)) {\n",
        "      return nGpuArchCoresPerSM[index].Cores;\n",
        "    }\n",
        "\n",
        "    index++;\n",
        "  }\n",
        "\n",
        "  // If we don't find the values, we default use the previous one\n",
        "  // to run properly\n",
        "  printf(\n",
        "      \"MapSMtoCores for SM %d.%d is undefined.\"\n",
        "      \"  Default to use %d Cores/SM\\n\",\n",
        "      major, minor, nGpuArchCoresPerSM[index - 1].Cores);\n",
        "  return nGpuArchCoresPerSM[index - 1].Cores;\n",
        "}\n",
        "\n",
        "inline const char* _ConvertSMVer2ArchName(int major, int minor) {\n",
        "  // Defines for GPU Architecture types (using the SM version to determine\n",
        "  // the GPU Arch name)\n",
        "  typedef struct {\n",
        "    int SM;  // 0xMm (hexidecimal notation), M = SM Major version,\n",
        "    // and m = SM minor version\n",
        "    const char* name;\n",
        "  } sSMtoArchName;\n",
        "\n",
        "  sSMtoArchName nGpuArchNameSM[] = {\n",
        "      {0x30, \"Kepler\"},\n",
        "      {0x32, \"Kepler\"},\n",
        "      {0x35, \"Kepler\"},\n",
        "      {0x37, \"Kepler\"},\n",
        "      {0x50, \"Maxwell\"},\n",
        "      {0x52, \"Maxwell\"},\n",
        "      {0x53, \"Maxwell\"},\n",
        "      {0x60, \"Pascal\"},\n",
        "      {0x61, \"Pascal\"},\n",
        "      {0x62, \"Pascal\"},\n",
        "      {0x70, \"Volta\"},\n",
        "      {0x72, \"Xavier\"},\n",
        "      {0x75, \"Turing\"},\n",
        "      {0x80, \"Ampere\"},\n",
        "      {0x86, \"Ampere\"},\n",
        "      {0x87, \"Ampere\"},\n",
        "      {0x89, \"Ada\"},\n",
        "      {0x90, \"Hopper\"},\n",
        "      {-1, \"Graphics Device\"}};\n",
        "\n",
        "  int index = 0;\n",
        "\n",
        "  while (nGpuArchNameSM[index].SM != -1) {\n",
        "    if (nGpuArchNameSM[index].SM == ((major << 4) + minor)) {\n",
        "      return nGpuArchNameSM[index].name;\n",
        "    }\n",
        "\n",
        "    index++;\n",
        "  }\n",
        "\n",
        "  // If we don't find the values, we default use the previous one\n",
        "  // to run properly\n",
        "  printf(\n",
        "      \"MapSMtoArchName for SM %d.%d is undefined.\"\n",
        "      \"  Default to use %s\\n\",\n",
        "      major, minor, nGpuArchNameSM[index - 1].name);\n",
        "  return nGpuArchNameSM[index - 1].name;\n",
        "}\n",
        "  // end of GPU Architecture definitions\n",
        "\n",
        "#ifdef __CUDA_RUNTIME_H__\n",
        "// General GPU Device CUDA Initialization\n",
        "inline int gpuDeviceInit(int devID) {\n",
        "  int device_count;\n",
        "  checkCudaErrors(cudaGetDeviceCount(&device_count));\n",
        "\n",
        "  if (device_count == 0) {\n",
        "    fprintf(stderr,\n",
        "            \"gpuDeviceInit() CUDA error: \"\n",
        "            \"no devices supporting CUDA.\\n\");\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "\n",
        "  if (devID < 0) {\n",
        "    devID = 0;\n",
        "  }\n",
        "\n",
        "  if (devID > device_count - 1) {\n",
        "    fprintf(stderr, \"\\n\");\n",
        "    fprintf(stderr, \">> %d CUDA capable GPU device(s) detected. <<\\n\",\n",
        "            device_count);\n",
        "    fprintf(stderr,\n",
        "            \">> gpuDeviceInit (-device=%d) is not a valid\"\n",
        "            \" GPU device. <<\\n\",\n",
        "            devID);\n",
        "    fprintf(stderr, \"\\n\");\n",
        "    return -devID;\n",
        "  }\n",
        "\n",
        "  int computeMode = -1, major = 0, minor = 0;\n",
        "  checkCudaErrors(cudaDeviceGetAttribute(&computeMode, cudaDevAttrComputeMode, devID));\n",
        "  checkCudaErrors(cudaDeviceGetAttribute(&major, cudaDevAttrComputeCapabilityMajor, devID));\n",
        "  checkCudaErrors(cudaDeviceGetAttribute(&minor, cudaDevAttrComputeCapabilityMinor, devID));\n",
        "  if (computeMode == cudaComputeModeProhibited) {\n",
        "    fprintf(stderr,\n",
        "            \"Error: device is running in <Compute Mode \"\n",
        "            \"Prohibited>, no threads can use cudaSetDevice().\\n\");\n",
        "    return -1;\n",
        "  }\n",
        "\n",
        "  if (major < 1) {\n",
        "    fprintf(stderr, \"gpuDeviceInit(): GPU device does not support CUDA.\\n\");\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "\n",
        "  checkCudaErrors(cudaSetDevice(devID));\n",
        "  printf(\"gpuDeviceInit() CUDA Device [%d]: \\\"%s\\n\", devID, _ConvertSMVer2ArchName(major, minor));\n",
        "\n",
        "  return devID;\n",
        "}\n",
        "\n",
        "// This function returns the best GPU (with maximum GFLOPS)\n",
        "inline int gpuGetMaxGflopsDeviceId() {\n",
        "  int current_device = 0, sm_per_multiproc = 0;\n",
        "  int max_perf_device = 0;\n",
        "  int device_count = 0;\n",
        "  int devices_prohibited = 0;\n",
        "\n",
        "  uint64_t max_compute_perf = 0;\n",
        "  checkCudaErrors(cudaGetDeviceCount(&device_count));\n",
        "\n",
        "  if (device_count == 0) {\n",
        "    fprintf(stderr,\n",
        "            \"gpuGetMaxGflopsDeviceId() CUDA error:\"\n",
        "            \" no devices supporting CUDA.\\n\");\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "\n",
        "  // Find the best CUDA capable GPU device\n",
        "  current_device = 0;\n",
        "\n",
        "  while (current_device < device_count) {\n",
        "    int computeMode = -1, major = 0, minor = 0;\n",
        "    checkCudaErrors(cudaDeviceGetAttribute(&computeMode, cudaDevAttrComputeMode, current_device));\n",
        "    checkCudaErrors(cudaDeviceGetAttribute(&major, cudaDevAttrComputeCapabilityMajor, current_device));\n",
        "    checkCudaErrors(cudaDeviceGetAttribute(&minor, cudaDevAttrComputeCapabilityMinor, current_device));\n",
        "\n",
        "    // If this GPU is not running on Compute Mode prohibited,\n",
        "    // then we can add it to the list\n",
        "    if (computeMode != cudaComputeModeProhibited) {\n",
        "      if (major == 9999 && minor == 9999) {\n",
        "        sm_per_multiproc = 1;\n",
        "      } else {\n",
        "        sm_per_multiproc =\n",
        "            _ConvertSMVer2Cores(major,  minor);\n",
        "      }\n",
        "      int multiProcessorCount = 0, clockRate = 0;\n",
        "      checkCudaErrors(cudaDeviceGetAttribute(&multiProcessorCount, cudaDevAttrMultiProcessorCount, current_device));\n",
        "      cudaError_t result = cudaDeviceGetAttribute(&clockRate, cudaDevAttrClockRate, current_device);\n",
        "      if (result != cudaSuccess) {\n",
        "        // If cudaDevAttrClockRate attribute is not supported we\n",
        "        // set clockRate as 1, to consider GPU with most SMs and CUDA Cores.\n",
        "        if(result == cudaErrorInvalidValue) {\n",
        "          clockRate = 1;\n",
        "        }\n",
        "        else {\n",
        "          fprintf(stderr, \"CUDA error at %s:%d code=%d(%s) \\n\", __FILE__, __LINE__,\n",
        "            static_cast<unsigned int>(result), _cudaGetErrorEnum(result));\n",
        "          exit(EXIT_FAILURE);\n",
        "        }\n",
        "      }\n",
        "      uint64_t compute_perf = (uint64_t)multiProcessorCount * sm_per_multiproc * clockRate;\n",
        "\n",
        "      if (compute_perf > max_compute_perf) {\n",
        "        max_compute_perf = compute_perf;\n",
        "        max_perf_device = current_device;\n",
        "      }\n",
        "    } else {\n",
        "      devices_prohibited++;\n",
        "    }\n",
        "\n",
        "    ++current_device;\n",
        "  }\n",
        "\n",
        "  if (devices_prohibited == device_count) {\n",
        "    fprintf(stderr,\n",
        "            \"gpuGetMaxGflopsDeviceId() CUDA error:\"\n",
        "            \" all devices have compute mode prohibited.\\n\");\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "\n",
        "  return max_perf_device;\n",
        "}\n",
        "\n",
        "// Initialization code to find the best CUDA Device\n",
        "inline int findCudaDevice(int argc, const char **argv) {\n",
        "  int devID = 0;\n",
        "\n",
        "  // If the command-line has a device number specified, use it\n",
        "  if (checkCmdLineFlag(argc, argv, \"device\")) {\n",
        "    devID = getCmdLineArgumentInt(argc, argv, \"device=\");\n",
        "\n",
        "    if (devID < 0) {\n",
        "      printf(\"Invalid command line parameter\\n \");\n",
        "      exit(EXIT_FAILURE);\n",
        "    } else {\n",
        "      devID = gpuDeviceInit(devID);\n",
        "\n",
        "      if (devID < 0) {\n",
        "        printf(\"exiting...\\n\");\n",
        "        exit(EXIT_FAILURE);\n",
        "      }\n",
        "    }\n",
        "  } else {\n",
        "    // Otherwise pick the device with highest Gflops/s\n",
        "    devID = gpuGetMaxGflopsDeviceId();\n",
        "    checkCudaErrors(cudaSetDevice(devID));\n",
        "    int major = 0, minor = 0;\n",
        "    checkCudaErrors(cudaDeviceGetAttribute(&major, cudaDevAttrComputeCapabilityMajor, devID));\n",
        "    checkCudaErrors(cudaDeviceGetAttribute(&minor, cudaDevAttrComputeCapabilityMinor, devID));\n",
        "    printf(\"GPU Device %d: \\\"%s\\\" with compute capability %d.%d\\n\\n\",\n",
        "           devID, _ConvertSMVer2ArchName(major, minor), major, minor);\n",
        "\n",
        "  }\n",
        "\n",
        "  return devID;\n",
        "}\n",
        "\n",
        "inline int findIntegratedGPU() {\n",
        "  int current_device = 0;\n",
        "  int device_count = 0;\n",
        "  int devices_prohibited = 0;\n",
        "\n",
        "  checkCudaErrors(cudaGetDeviceCount(&device_count));\n",
        "\n",
        "  if (device_count == 0) {\n",
        "    fprintf(stderr, \"CUDA error: no devices supporting CUDA.\\n\");\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "\n",
        "  // Find the integrated GPU which is compute capable\n",
        "  while (current_device < device_count) {\n",
        "    int computeMode = -1, integrated = -1;\n",
        "    checkCudaErrors(cudaDeviceGetAttribute(&computeMode, cudaDevAttrComputeMode, current_device));\n",
        "    checkCudaErrors(cudaDeviceGetAttribute(&integrated, cudaDevAttrIntegrated, current_device));\n",
        "    // If GPU is integrated and is not running on Compute Mode prohibited,\n",
        "    // then cuda can map to GLES resource\n",
        "    if (integrated && (computeMode != cudaComputeModeProhibited)) {\n",
        "      checkCudaErrors(cudaSetDevice(current_device));\n",
        "\n",
        "      int major = 0, minor = 0;\n",
        "      checkCudaErrors(cudaDeviceGetAttribute(&major, cudaDevAttrComputeCapabilityMajor, current_device));\n",
        "      checkCudaErrors(cudaDeviceGetAttribute(&minor, cudaDevAttrComputeCapabilityMinor, current_device));\n",
        "      printf(\"GPU Device %d: \\\"%s\\\" with compute capability %d.%d\\n\\n\",\n",
        "             current_device, _ConvertSMVer2ArchName(major, minor), major, minor);\n",
        "\n",
        "      return current_device;\n",
        "    } else {\n",
        "      devices_prohibited++;\n",
        "    }\n",
        "\n",
        "    current_device++;\n",
        "  }\n",
        "\n",
        "  if (devices_prohibited == device_count) {\n",
        "    fprintf(stderr,\n",
        "            \"CUDA error:\"\n",
        "            \" No GLES-CUDA Interop capable GPU found.\\n\");\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "\n",
        "  return -1;\n",
        "}\n",
        "\n",
        "// General check for CUDA GPU SM Capabilities\n",
        "inline bool checkCudaCapabilities(int major_version, int minor_version) {\n",
        "  int dev;\n",
        "  int major = 0, minor = 0;\n",
        "\n",
        "  checkCudaErrors(cudaGetDevice(&dev));\n",
        "  checkCudaErrors(cudaDeviceGetAttribute(&major, cudaDevAttrComputeCapabilityMajor, dev));\n",
        "  checkCudaErrors(cudaDeviceGetAttribute(&minor, cudaDevAttrComputeCapabilityMinor, dev));\n",
        "\n",
        "  if ((major > major_version) ||\n",
        "      (major == major_version &&\n",
        "       minor >= minor_version)) {\n",
        "    printf(\"  Device %d: <%16s >, Compute SM %d.%d detected\\n\", dev,\n",
        "           _ConvertSMVer2ArchName(major, minor), major, minor);\n",
        "    return true;\n",
        "  } else {\n",
        "    printf(\n",
        "        \"  No GPU device was found that can support \"\n",
        "        \"CUDA compute capability %d.%d.\\n\",\n",
        "        major_version, minor_version);\n",
        "    return false;\n",
        "  }\n",
        "}\n",
        "#endif\n",
        "\n",
        "  // end of CUDA Helper Functions\n",
        "\n",
        "#endif  // COMMON_HELPER_CUDA_H_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A9DCGnPGia5",
        "outputId": "ca444636-e5ef-4406-ac80-94a5b15ccb07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing helper_cuda.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile helper_string.h\n",
        "/* Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n",
        " *\n",
        " * Redistribution and use in source and binary forms, with or without\n",
        " * modification, are permitted provided that the following conditions\n",
        " * are met:\n",
        " *  * Redistributions of source code must retain the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer.\n",
        " *  * Redistributions in binary form must reproduce the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer in the\n",
        " *    documentation and/or other materials provided with the distribution.\n",
        " *  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
        " *    contributors may be used to endorse or promote products derived\n",
        " *    from this software without specific prior written permission.\n",
        " *\n",
        " * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
        " * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        " * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
        " * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
        " * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
        " * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
        " * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
        " * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
        " * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        " * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        " * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        " */\n",
        "\n",
        "// These are helper functions for the SDK samples (string parsing, timers, etc)\n",
        "#ifndef COMMON_HELPER_STRING_H_\n",
        "#define COMMON_HELPER_STRING_H_\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <fstream>\n",
        "#include <string>\n",
        "\n",
        "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
        "#ifndef _CRT_SECURE_NO_DEPRECATE\n",
        "#define _CRT_SECURE_NO_DEPRECATE\n",
        "#endif\n",
        "#ifndef STRCASECMP\n",
        "#define STRCASECMP _stricmp\n",
        "#endif\n",
        "#ifndef STRNCASECMP\n",
        "#define STRNCASECMP _strnicmp\n",
        "#endif\n",
        "#ifndef STRCPY\n",
        "#define STRCPY(sFilePath, nLength, sPath) strcpy_s(sFilePath, nLength, sPath)\n",
        "#endif\n",
        "\n",
        "#ifndef FOPEN\n",
        "#define FOPEN(fHandle, filename, mode) fopen_s(&fHandle, filename, mode)\n",
        "#endif\n",
        "#ifndef FOPEN_FAIL\n",
        "#define FOPEN_FAIL(result) (result != 0)\n",
        "#endif\n",
        "#ifndef SSCANF\n",
        "#define SSCANF sscanf_s\n",
        "#endif\n",
        "#ifndef SPRINTF\n",
        "#define SPRINTF sprintf_s\n",
        "#endif\n",
        "#else  // Linux Includes\n",
        "#include <string.h>\n",
        "#include <strings.h>\n",
        "\n",
        "#ifndef STRCASECMP\n",
        "#define STRCASECMP strcasecmp\n",
        "#endif\n",
        "#ifndef STRNCASECMP\n",
        "#define STRNCASECMP strncasecmp\n",
        "#endif\n",
        "#ifndef STRCPY\n",
        "#define STRCPY(sFilePath, nLength, sPath) strcpy(sFilePath, sPath)\n",
        "#endif\n",
        "\n",
        "#ifndef FOPEN\n",
        "#define FOPEN(fHandle, filename, mode) (fHandle = fopen(filename, mode))\n",
        "#endif\n",
        "#ifndef FOPEN_FAIL\n",
        "#define FOPEN_FAIL(result) (result == NULL)\n",
        "#endif\n",
        "#ifndef SSCANF\n",
        "#define SSCANF sscanf\n",
        "#endif\n",
        "#ifndef SPRINTF\n",
        "#define SPRINTF sprintf\n",
        "#endif\n",
        "#endif\n",
        "\n",
        "#ifndef EXIT_WAIVED\n",
        "#define EXIT_WAIVED 2\n",
        "#endif\n",
        "\n",
        "// CUDA Utility Helper Functions\n",
        "inline int stringRemoveDelimiter(char delimiter, const char *string) {\n",
        "  int string_start = 0;\n",
        "\n",
        "  while (string[string_start] == delimiter) {\n",
        "    string_start++;\n",
        "  }\n",
        "\n",
        "  if (string_start >= static_cast<int>(strlen(string) - 1)) {\n",
        "    return 0;\n",
        "  }\n",
        "\n",
        "  return string_start;\n",
        "}\n",
        "\n",
        "inline int getFileExtension(char *filename, char **extension) {\n",
        "  int string_length = static_cast<int>(strlen(filename));\n",
        "\n",
        "  while (filename[string_length--] != '.') {\n",
        "    if (string_length == 0) break;\n",
        "  }\n",
        "\n",
        "  if (string_length > 0) string_length += 2;\n",
        "\n",
        "  if (string_length == 0)\n",
        "    *extension = NULL;\n",
        "  else\n",
        "    *extension = &filename[string_length];\n",
        "\n",
        "  return string_length;\n",
        "}\n",
        "\n",
        "inline bool checkCmdLineFlag(const int argc, const char **argv,\n",
        "                             const char *string_ref) {\n",
        "  bool bFound = false;\n",
        "\n",
        "  if (argc >= 1) {\n",
        "    for (int i = 1; i < argc; i++) {\n",
        "      int string_start = stringRemoveDelimiter('-', argv[i]);\n",
        "      const char *string_argv = &argv[i][string_start];\n",
        "\n",
        "      const char *equal_pos = strchr(string_argv, '=');\n",
        "      int argv_length = static_cast<int>(\n",
        "          equal_pos == 0 ? strlen(string_argv) : equal_pos - string_argv);\n",
        "\n",
        "      int length = static_cast<int>(strlen(string_ref));\n",
        "\n",
        "      if (length == argv_length &&\n",
        "          !STRNCASECMP(string_argv, string_ref, length)) {\n",
        "        bFound = true;\n",
        "        continue;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  return bFound;\n",
        "}\n",
        "\n",
        "// This function wraps the CUDA Driver API into a template function\n",
        "template <class T>\n",
        "inline bool getCmdLineArgumentValue(const int argc, const char **argv,\n",
        "                                    const char *string_ref, T *value) {\n",
        "  bool bFound = false;\n",
        "\n",
        "  if (argc >= 1) {\n",
        "    for (int i = 1; i < argc; i++) {\n",
        "      int string_start = stringRemoveDelimiter('-', argv[i]);\n",
        "      const char *string_argv = &argv[i][string_start];\n",
        "      int length = static_cast<int>(strlen(string_ref));\n",
        "\n",
        "      if (!STRNCASECMP(string_argv, string_ref, length)) {\n",
        "        if (length + 1 <= static_cast<int>(strlen(string_argv))) {\n",
        "          int auto_inc = (string_argv[length] == '=') ? 1 : 0;\n",
        "          *value = (T)atoi(&string_argv[length + auto_inc]);\n",
        "        }\n",
        "\n",
        "        bFound = true;\n",
        "        i = argc;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  return bFound;\n",
        "}\n",
        "\n",
        "inline int getCmdLineArgumentInt(const int argc, const char **argv,\n",
        "                                 const char *string_ref) {\n",
        "  bool bFound = false;\n",
        "  int value = -1;\n",
        "\n",
        "  if (argc >= 1) {\n",
        "    for (int i = 1; i < argc; i++) {\n",
        "      int string_start = stringRemoveDelimiter('-', argv[i]);\n",
        "      const char *string_argv = &argv[i][string_start];\n",
        "      int length = static_cast<int>(strlen(string_ref));\n",
        "\n",
        "      if (!STRNCASECMP(string_argv, string_ref, length)) {\n",
        "        if (length + 1 <= static_cast<int>(strlen(string_argv))) {\n",
        "          int auto_inc = (string_argv[length] == '=') ? 1 : 0;\n",
        "          value = atoi(&string_argv[length + auto_inc]);\n",
        "        } else {\n",
        "          value = 0;\n",
        "        }\n",
        "\n",
        "        bFound = true;\n",
        "        continue;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (bFound) {\n",
        "    return value;\n",
        "  } else {\n",
        "    return 0;\n",
        "  }\n",
        "}\n",
        "\n",
        "inline float getCmdLineArgumentFloat(const int argc, const char **argv,\n",
        "                                     const char *string_ref) {\n",
        "  bool bFound = false;\n",
        "  float value = -1;\n",
        "\n",
        "  if (argc >= 1) {\n",
        "    for (int i = 1; i < argc; i++) {\n",
        "      int string_start = stringRemoveDelimiter('-', argv[i]);\n",
        "      const char *string_argv = &argv[i][string_start];\n",
        "      int length = static_cast<int>(strlen(string_ref));\n",
        "\n",
        "      if (!STRNCASECMP(string_argv, string_ref, length)) {\n",
        "        if (length + 1 <= static_cast<int>(strlen(string_argv))) {\n",
        "          int auto_inc = (string_argv[length] == '=') ? 1 : 0;\n",
        "          value = static_cast<float>(atof(&string_argv[length + auto_inc]));\n",
        "        } else {\n",
        "          value = 0.f;\n",
        "        }\n",
        "\n",
        "        bFound = true;\n",
        "        continue;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (bFound) {\n",
        "    return value;\n",
        "  } else {\n",
        "    return 0;\n",
        "  }\n",
        "}\n",
        "\n",
        "inline bool getCmdLineArgumentString(const int argc, const char **argv,\n",
        "                                     const char *string_ref,\n",
        "                                     char **string_retval) {\n",
        "  bool bFound = false;\n",
        "\n",
        "  if (argc >= 1) {\n",
        "    for (int i = 1; i < argc; i++) {\n",
        "      int string_start = stringRemoveDelimiter('-', argv[i]);\n",
        "      char *string_argv = const_cast<char *>(&argv[i][string_start]);\n",
        "      int length = static_cast<int>(strlen(string_ref));\n",
        "\n",
        "      if (!STRNCASECMP(string_argv, string_ref, length)) {\n",
        "        *string_retval = &string_argv[length + 1];\n",
        "        bFound = true;\n",
        "        continue;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (!bFound) {\n",
        "    *string_retval = NULL;\n",
        "  }\n",
        "\n",
        "  return bFound;\n",
        "}\n",
        "\n",
        "//////////////////////////////////////////////////////////////////////////////\n",
        "//! Find the path for a file assuming that\n",
        "//! files are found in the searchPath.\n",
        "//!\n",
        "//! @return the path if succeeded, otherwise 0\n",
        "//! @param filename         name of the file\n",
        "//! @param executable_path  optional absolute path of the executable\n",
        "//////////////////////////////////////////////////////////////////////////////\n",
        "inline char *sdkFindFilePath(const char *filename,\n",
        "                             const char *executable_path) {\n",
        "  // <executable_name> defines a variable that is replaced with the name of the\n",
        "  // executable\n",
        "\n",
        "  // Typical relative search paths to locate needed companion files (e.g. sample\n",
        "  // input data, or JIT source files) The origin for the relative search may be\n",
        "  // the .exe file, a .bat file launching an .exe, a browser .exe launching the\n",
        "  // .exe or .bat, etc\n",
        "  const char *searchPath[] = {\n",
        "      \"./\",                                           // same dir\n",
        "      \"./data/\",                                      // same dir\n",
        "\n",
        "      \"../../../../Samples/<executable_name>/\",       // up 4 in tree\n",
        "      \"../../../Samples/<executable_name>/\",          // up 3 in tree\n",
        "      \"../../Samples/<executable_name>/\",             // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/<executable_name>/data/\",  // up 4 in tree\n",
        "      \"../../../Samples/<executable_name>/data/\",     // up 3 in tree\n",
        "      \"../../Samples/<executable_name>/data/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/0_Introduction/<executable_name>/\",  // up 4 in tree\n",
        "      \"../../../Samples/0_Introduction/<executable_name>/\",     // up 3 in tree\n",
        "      \"../../Samples/0_Introduction/<executable_name>/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/1_Utilities/<executable_name>/\",  // up 4 in tree\n",
        "      \"../../../Samples/1_Utilities/<executable_name>/\",     // up 3 in tree\n",
        "      \"../../Samples/1_Utilities/<executable_name>/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/2_Concepts_and_Techniques/<executable_name>/\",  // up 4 in tree\n",
        "      \"../../../Samples/2_Concepts_and_Techniques/<executable_name>/\",     // up 3 in tree\n",
        "      \"../../Samples/2_Concepts_and_Techniques/<executable_name>/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/3_CUDA_Features/<executable_name>/\",  // up 4 in tree\n",
        "      \"../../../Samples/3_CUDA_Features/<executable_name>/\",     // up 3 in tree\n",
        "      \"../../Samples/3_CUDA_Features/<executable_name>/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/4_CUDA_Libraries/<executable_name>/\",  // up 4 in tree\n",
        "      \"../../../Samples/4_CUDA_Libraries/<executable_name>/\",     // up 3 in tree\n",
        "      \"../../Samples/4_CUDA_Libraries/<executable_name>/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/5_Domain_Specific/<executable_name>/\",  // up 4 in tree\n",
        "      \"../../../Samples/5_Domain_Specific/<executable_name>/\",     // up 3 in tree\n",
        "      \"../../Samples/5_Domain_Specific/<executable_name>/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/6_Performance/<executable_name>/\",  // up 4 in tree\n",
        "      \"../../../Samples/6_Performance/<executable_name>/\",     // up 3 in tree\n",
        "      \"../../Samples/6_Performance/<executable_name>/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/0_Introduction/<executable_name>/data/\",  // up 4 in tree\n",
        "      \"../../../Samples/0_Introduction/<executable_name>/data/\",     // up 3 in tree\n",
        "      \"../../Samples/0_Introduction/<executable_name>/data/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/1_Utilities/<executable_name>/data/\",  // up 4 in tree\n",
        "      \"../../../Samples/1_Utilities/<executable_name>/data/\",     // up 3 in tree\n",
        "      \"../../Samples/1_Utilities/<executable_name>/data/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/2_Concepts_and_Techniques/<executable_name>/data/\",  // up 4 in tree\n",
        "      \"../../../Samples/2_Concepts_and_Techniques/<executable_name>/data/\",     // up 3 in tree\n",
        "      \"../../Samples/2_Concepts_and_Techniques/<executable_name>/data/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/3_CUDA_Features/<executable_name>/data/\",  // up 4 in tree\n",
        "      \"../../../Samples/3_CUDA_Features/<executable_name>/data/\",     // up 3 in tree\n",
        "      \"../../Samples/3_CUDA_Features/<executable_name>/data/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/4_CUDA_Libraries/<executable_name>/data/\",  // up 4 in tree\n",
        "      \"../../../Samples/4_CUDA_Libraries/<executable_name>/data/\",     // up 3 in tree\n",
        "      \"../../Samples/4_CUDA_Libraries/<executable_name>/data/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/5_Domain_Specific/<executable_name>/data/\",  // up 4 in tree\n",
        "      \"../../../Samples/5_Domain_Specific/<executable_name>/data/\",     // up 3 in tree\n",
        "      \"../../Samples/5_Domain_Specific/<executable_name>/data/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Samples/6_Performance/<executable_name>/data/\",  // up 4 in tree\n",
        "      \"../../../Samples/6_Performance/<executable_name>/data/\",     // up 3 in tree\n",
        "      \"../../Samples/6_Performance/<executable_name>/data/\",        // up 2 in tree\n",
        "\n",
        "      \"../../../../Common/data/\",                     // up 4 in tree\n",
        "      \"../../../Common/data/\",                        // up 3 in tree\n",
        "      \"../../Common/data/\"                            // up 2 in tree\n",
        "  };\n",
        "\n",
        "  // Extract the executable name\n",
        "  std::string executable_name;\n",
        "\n",
        "  if (executable_path != 0) {\n",
        "    executable_name = std::string(executable_path);\n",
        "\n",
        "#if defined(WIN32) || defined(_WIN32) || defined(WIN64) || defined(_WIN64)\n",
        "    // Windows path delimiter\n",
        "    size_t delimiter_pos = executable_name.find_last_of('\\\\');\n",
        "    executable_name.erase(0, delimiter_pos + 1);\n",
        "\n",
        "    if (executable_name.rfind(\".exe\") != std::string::npos) {\n",
        "      // we strip .exe, only if the .exe is found\n",
        "      executable_name.resize(executable_name.size() - 4);\n",
        "    }\n",
        "\n",
        "#else\n",
        "    // Linux & OSX path delimiter\n",
        "    size_t delimiter_pos = executable_name.find_last_of('/');\n",
        "    executable_name.erase(0, delimiter_pos + 1);\n",
        "#endif\n",
        "  }\n",
        "\n",
        "  // Loop over all search paths and return the first hit\n",
        "  for (unsigned int i = 0; i < sizeof(searchPath) / sizeof(char *); ++i) {\n",
        "    std::string path(searchPath[i]);\n",
        "    size_t executable_name_pos = path.find(\"<executable_name>\");\n",
        "\n",
        "    // If there is executable_name variable in the searchPath\n",
        "    // replace it with the value\n",
        "    if (executable_name_pos != std::string::npos) {\n",
        "      if (executable_path != 0) {\n",
        "        path.replace(executable_name_pos, strlen(\"<executable_name>\"),\n",
        "                     executable_name);\n",
        "      } else {\n",
        "        // Skip this path entry if no executable argument is given\n",
        "        continue;\n",
        "      }\n",
        "    }\n",
        "\n",
        "#ifdef _DEBUG\n",
        "    printf(\"sdkFindFilePath <%s> in %s\\n\", filename, path.c_str());\n",
        "#endif\n",
        "\n",
        "    // Test if the file exists\n",
        "    path.append(filename);\n",
        "    FILE *fp;\n",
        "    FOPEN(fp, path.c_str(), \"rb\");\n",
        "\n",
        "    if (fp != NULL) {\n",
        "      fclose(fp);\n",
        "      // File found\n",
        "      // returning an allocated array here for backwards compatibility reasons\n",
        "      char *file_path = reinterpret_cast<char *>(malloc(path.length() + 1));\n",
        "      STRCPY(file_path, path.length() + 1, path.c_str());\n",
        "      return file_path;\n",
        "    }\n",
        "\n",
        "    if (fp) {\n",
        "      fclose(fp);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // File not found\n",
        "  printf(\"\\nerror: sdkFindFilePath: file <%s> not found!\\n\", filename);\n",
        "  return 0;\n",
        "}\n",
        "\n",
        "#endif  // COMMON_HELPER_STRING_H_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaVPRquTGxnH",
        "outputId": "eeaaa4b3-7aaf-4ae4-ddb6-9c2165e13118"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing helper_string.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile deviceQuery\n",
        "!nvcc -o deviceQuery -I/content deviceQuery.cu\n"
      ],
      "metadata": {
        "id": "EKUYU6oIG_n6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./deviceQuery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAVo2tlzIONp",
        "outputId": "a13b27b7-a4b7-4238-ca66-c2a224bfe19d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla T4\"\n",
            "  CUDA Driver Version / Runtime Version          12.2 / 12.2\n",
            "  CUDA Capability Major/Minor version number:    7.5\n",
            "  Total amount of global memory:                 15102 MBytes (15835660288 bytes)\n",
            "  (040) Multiprocessors, (064) CUDA Cores/MP:    2560 CUDA Cores\n",
            "  GPU Max Clock rate:                            1590 MHz (1.59 GHz)\n",
            "  Memory Clock rate:                             5001 Mhz\n",
            "  Memory Bus Width:                              256-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total shared memory per multiprocessor:        65536 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  1024\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Managed Memory:                Yes\n",
            "  Device supports Compute Preemption:            Yes\n",
            "  Supports Cooperative Kernel Launch:            Yes\n",
            "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.2, CUDA Runtime Version = 12.2, NumDevs = 1\n",
            "Result = PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AzlgVuGZHJrN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}